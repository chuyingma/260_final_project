---
title: "Classification -- combine all approaches and comparison"
author: "Chuying Ma"
date: "11/27/2017"
output: html_document
---

##BST 260 Final Project
##Group: Chuying Ma, Jingjing Tang, Jie Yin, Xuewei Zhang

We studied our county level data from year 2000 to 2010 using classification approaches such as Logistic Regression (Logit. Reg.), K-nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and Random Forest (RF) based on 16 chosen features. And we want to compare their performance in the end


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(ggplot2)
library(readr)
library(dplyr)
library(pROC)
library(caret)
library(randomForest)
data_all = read_csv("data_3_expo.csv")
library(dslabs)
ds_theme_set()
```

##Part 1. exploration and dividing classes

exploratory analysis for different classes:
```{r,warning=FALSE}
data_all %>%
  ggplot(aes(menthlth)) +
  geom_histogram(fill = "darkslateblue",color = "black") + 
  ggtitle("Distribution of average numbers of days mental health not good on county level")
```

First, we divided our county level data based on average number of days mental health not good during the past 30 days (menthlth) into two categories: (1) less than or equal to 10 days mental health not good (class 0); (2) larger than 10 days but smaller than 31 days (class 1).

```{r,eval=FALSE}
mean(data_all$menthlth <= 10,na.rm = T)
mean(data_all$menthlth > 10,na.rm = T)
```


divide into 2 classes, which are "good" -- 0, "bad" -- 1
```{r,eval=FALSE}
data_all$mental_class[data_all$menthlth > 0 & data_all$menthlth <= 10] <- 0
data_all$mental_class[data_all$menthlth > 10 & data_all$menthlth <= 30] <- 1
```

divide the dataset into training and test:
```{r,eval=FALSE}

dat_class = data_all %>%
  select(-c(fips,state,county,year,heartattack,ACheartdis,stroke,asthma,menthlth))

set.seed(1)
n_test <- round(nrow(dat_class) / 10)
test_indices <- sample(1:nrow(dat_class), n_test, replace=FALSE)
test_dat <- dat_class[test_indices,]
train_dat <- dat_class[-test_indices,]
```

try logistic (try different cutoff):

```{r,,eval=FALSE}
fit <- train_dat %>% 
  glm(mental_class~., data=., family = "binomial")
test_dat_com = test_dat[complete.cases(test_dat),]
p_hat_logit <- predict(fit, newdata = test_dat_com, type="response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0)
confusionMatrix(data = y_hat_logit, reference = test_dat_com$mental_class)
summary(fit)
```


try knn (try different k):
```{r,,eval=FALSE}
knn_fit <- knn3(mental_class~.,data = train_dat)
test_dat_com = test_dat[complete.cases(test_dat),]
f_hat <- predict(knn_fit, newdata = test_dat_com)[,2]
tab <- table(pred=round(f_hat), truth=test_dat_com$mental_class)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```


try random forest:

prepare train and test set:
```{r,,eval=FALSE}
data_all = read_csv("data_3_expo.csv")
```


```{r,,eval=FALSE}
data_all$mental_class[data_all$menthlth > 0 & data_all$menthlth <= 10] <- 0
data_all$mental_class[data_all$menthlth > 10 & data_all$menthlth <= 30] <- 1
```

divide the dataset into training and test:
```{r,eval=FALSE}

dat_class = data_all %>%
  select(c(income7,employed,out_of_work,homemaker,student,hlthplan,exercise,smoke,drink, genhlth_fa,genhlth_po,qlactlm,pm2.5,ozone,greenness,mental_class))

set.seed(1993)
n_test <- round(nrow(dat_class)/6)
test_indices <- sample(1:nrow(dat_class), n_test, replace=FALSE)
test_dat <- dat_class[test_indices,]
train_dat <- dat_class[-test_indices,]
```

```{r,,eval=FALSE}
write_csv(train_dat,"train.csv")
write_csv(test_dat,"test.csv")
```

try random forest for new generated test and train dataset:
```{r}
test_dat = read_csv("test.csv")
train_dat = read_csv("train.csv")
```

viewing the class in 2D via our 2 exposures of interest (ozone and greenness):
```{r,warning=FALSE}
p <- train_dat %>% 
  filter(!is.na(mental_class)) %>%
  ggplot(aes(ozone, greenness)) +
  geom_point(aes(color = factor(mental_class))) +
  ggtitle("Viewing the mental_class in 2D plot with ozone and greenness")
colors <- c("lightcoral", "cornflowerblue")
p + scale_color_manual(values=colors)
```

try CART:
```{r}
library(stringr)
library(lubridate)
library(tidyr)
library(XML)
library(tree)
fit1 <- train_dat %>%
  filter(!is.na(mental_class)) %>%
  mutate(mental_class = factor(mental_class)) %>%
  tree(mental_class ~ ., data = .)

plot(fit1)
text(fit1)

```

##Part 2. Random Forest

```{r}
library(randomForest)
train_dat_com = train_dat[complete.cases(train_dat),]
train_dat_com$mental_class = as.factor(train_dat_com$mental_class)


fit2 <- randomForest(mental_class ~., data = train_dat_com, ntree=500, nodesize = 15, mtry=1)
```

```{r}
test_dat_com = test_dat[complete.cases(test_dat),]
test_dat_com$mental_class = as.factor(test_dat_com$mental_class)
pred <- predict(fit2, newdata = test_dat_com, type = "class")


tab <- table(pred, test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

```{r,,eval=FALSE}
fit3 <- randomForest(mental_class ~., data = train_dat_com, ntree=700, nodesize = 10, mtry=1)

pred <- predict(fit3, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

try to choose best node size:
```{r,eval=FALSE}
node_size = seq(1,20,1)
accuracy = rep(0,20)
for(i in 1:20) {
  fit = randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = node_size[i], mtry=1)
  pred <- predict(fit, newdata = test_dat_com, type = "class")
  tab <- table(pred = pred, true = test_dat_com$mental_class)
  accuracy[i] = confusionMatrix(tab)$overall["Accuracy"]
}
node_size[which.max(accuracy)]
```

```{r,eval=FALSE}
set.seed(1)
fit1 <- randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = 13, mtry=3)

pred <- predict(fit1, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

try to choose best mtry:

```{r,eval=FALSE}
set.seed(1)
node_size = seq(1,20,1)
mtry_n = seq(1,15,1)
accuracy = c()
for(i in 1:20) {
  for (j in 1:15) {
    fit = randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = node_size[i], mtry=mtry_n[j])
  pred <- predict(fit, newdata = test_dat_com, type = "class")
  tab <- table(pred = pred, true = test_dat_com$mental_class)
  accuracy = c(confusionMatrix(tab)$overall["Accuracy"],accuracy)
  }
}
```

I decided to use nodesize = 15, mtry=5:

```{r}
set.seed(1)
fit2 <- randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = 15, mtry=5)

pred <- predict(fit2, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]

pred <- predict(fit2, newdata = test_dat_com,type = "prob")


tree.prob = pred[,2]
```

##Conclusion
the accracy rate is 0.7564, which is very close to LDA


In order to find the important variables in the random forest:

```{r,eval=FALSE}
library(knitr)
variable_importance <- importance(fit1) 
tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
  arrange(desc(Gini))
```

```{r,eval=FALSE}
ds_theme_set()
tmp %>%
  ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
  geom_bar(stat='identity',fill = "darkseagreen2") +
  coord_flip() + xlab("Feature") +
  theme(axis.text=element_text(size=8)) + 
  ggtitle("mean feature importances for random forest classifer")
```

##Conclusion
The plot of mean feature importances indicate that genhlth_po, drink, exercise are the top 3 important features for these classifiers to predict which category the average number of days mental health not good during the past 30 days falls. 


##Part 3. Comparison among approaches

try to use lda and qda for classification:
```{r}
library(MASS)
lda.fit=lda(mental_class~.,data=train_dat_com)
lda.fit
lda.pred=predict(lda.fit, test_dat_com)
lda.class=lda.pred$class 
tab <- table(pred = lda.class, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
#0.756371 

lda.prob = lda.pred$posterior[,1]
```

```{r}

qda.fit=qda(mental_class~.,data=train_dat_com)
qda.fit

qda.pred=predict(qda.fit, test_dat_com)

qda.class=qda.pred$class 
tab = table(qda.class,test_dat_com$mental_class)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]

qda.prob = qda.pred$posterior[,1]

#0.7319062 
```

Logistic Regression (2 exposure)
```{r}
glm_fit <- train_dat_com %>% 
  mutate(y = as.numeric(mental_class == 1)) %>%
  glm(y ~ income7 + employed + out_of_work + homemaker + student + hlthplan + exercise + smoke + drink + genhlth_fa + genhlth_po + qlactlm + pm2.5 + ozone + greenness, data=., family = "binomial")
```

obtain prediction using the predict function:

```{r}
p_hat_logit <- predict(glm_fit, newdata = test_dat_com, type="response")
```

obtain predictions

```{r}
y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0)
confusionMatrix(data = y_hat_logit, reference = test_dat_com$mental_class)
#0.7528 
```



KNN

```{r,eval=FALSE}
accuracy <- c()
for(x in 1:100){
knn_fit <- knn3(mental_class~. ,data = na.omit(train_set), k=x)
f_hat <- predict(knn_fit, newdata = na.omit(test_set))[,2]
tab <- table(pred=round(f_hat), truth=na.omit(test_set)$mental_class)
accuracy[x] <- confusionMatrix(tab)$overall["Accuracy"]
}
x <- 1:100
data.frame(x, accuracy) %>% ggplot(aes(x, accuracy))+
  geom_point(aes())+
  geom_line()+
  geom_vline(xintercept=x[which.max(accuracy)], col = "red")
```

so, k=35

```{r}
knn_fit <- knn3(mental_class~. ,data = na.omit(train_dat_com), k=35)
f_hat <- predict(knn_fit, newdata = na.omit(test_dat_com))[,2]
tab <- table(pred=round(f_hat), truth=na.omit(test_dat_com)$mental_class)
confusionMatrix(tab)$overall["Accuracy"]
```


Draw the ROC curve to compare:

```{r}


roc_ran <- roc(test_dat_com$mental_class,tree.prob)
auc(roc_ran)
#0.8106
roc_qda <- roc(test_dat_com$mental_class, qda.prob)
auc(roc_qda)
#0.7757
roc_lda = roc(test_dat_com$mental_class, lda.prob)
auc(roc_lda)
#0.79
roc_lg = roc(test_dat_com$mental_class, p_hat_logit)
auc(roc_lg)
#0.7919

roc_knn = roc(test_dat_com$mental_class, f_hat)
auc(roc_knn)
#0.7852

plot(roc_ran,col = "brown1")
lines(roc_qda, col = "blue1")
lines(roc_lda,col = "darkgoldenrod")
lines(roc_lg , col = "darkorchid")
lines(roc_knn, col = "chartreuse1")
legend(0.4,0.5,c("Random Forest","QDA","LDA","LG","KNN"),col=c("brown1","blue1","darkgoldenrod","darkorchid","chartreuse1"),lty=1)
```

##Conclusions for the classification

As can be seen from the combined ROC curves and the table above, all those five techniques perform relatively similar. However, we are more willing to detect potential mental problems as more sensitive as possible. Under this consideration, larger AUC is better. So, Random Forest (RF) is slightly better than other approaches.



