---
title: "mental_advanced"
author: "Chuying Ma"
date: "11/27/2017"
output: html_document
---

BST 260 Final Project
Group: Chuying Ma, Jingjing Tang, Jie Yin, Xuewei Zhang

We studied our county level data from year 2000 to 2010 using classification approaches such as Logistic Regression (Logit. Reg.), K-nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA) and Random Forest (RF) based on 16 chosen features.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

try advanced techniques for classification:
```{r}
library(ggplot2)
library(readr)
library(dplyr)
library(pROC)
library(caret)
library(randomForest)
data_all = read_csv("data_3_expo.csv")
library(dslabs)
ds_theme_set()
```

classification:

exploratory analysis for different classes:
```{r,warning=FALSE}
data_all %>%
  ggplot(aes(menthlth)) +
  geom_histogram(fill = "darkslateblue",color = "black") + 
  ggtitle("Distribution of average numbers of days mental health not good on county level")
```

First, we divided our county level data based on average number of days mental health not good during the past 30 days (menthlth) into two categories: (1) less than or equal to 10 days mental health not good (class 0); (2) larger than 10 days but smaller than 31 days (class 1).

```{r,eval=FALSE}
mean(data_all$menthlth <= 10,na.rm = T)
mean(data_all$menthlth > 10,na.rm = T)
```


divide into 2 classes, which are "good" -- 0, "bad" -- 1
```{r,eval=FALSE}
data_all$mental_class[data_all$menthlth > 0 & data_all$menthlth <= 10] <- 0
data_all$mental_class[data_all$menthlth > 10 & data_all$menthlth <= 30] <- 1
```

divide the dataset into training and test:
```{r,eval=FALSE}

dat_class = data_all %>%
  select(-c(fips,state,county,year,heartattack,ACheartdis,stroke,asthma,menthlth))

set.seed(1)
n_test <- round(nrow(dat_class) / 10)
test_indices <- sample(1:nrow(dat_class), n_test, replace=FALSE)
test_dat <- dat_class[test_indices,]
train_dat <- dat_class[-test_indices,]
```

try logistic (try different cutoff):

```{r,,eval=FALSE}
fit <- train_dat %>% 
  glm(mental_class~., data=., family = "binomial")
test_dat_com = test_dat[complete.cases(test_dat),]
p_hat_logit <- predict(fit, newdata = test_dat_com, type="response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0)
confusionMatrix(data = y_hat_logit, reference = test_dat_com$mental_class)
summary(fit)
```


try knn (try different k):
```{r,,eval=FALSE}
knn_fit <- knn3(mental_class~.,data = train_dat)
test_dat_com = test_dat[complete.cases(test_dat),]
f_hat <- predict(knn_fit, newdata = test_dat_com)[,2]
tab <- table(pred=round(f_hat), truth=test_dat_com$mental_class)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```


try random forest:

prepare train and test set:
```{r,,eval=FALSE}
data_all = read_csv("data_3_expo.csv")
```


```{r,,eval=FALSE}
data_all$mental_class[data_all$menthlth > 0 & data_all$menthlth <= 10] <- 0
data_all$mental_class[data_all$menthlth > 10 & data_all$menthlth <= 30] <- 1
```

divide the dataset into training and test:
```{r,eval=FALSE}

dat_class = data_all %>%
  select(c(income7,employed,out_of_work,homemaker,student,hlthplan,exercise,smoke,drink, genhlth_fa,genhlth_po,qlactlm,pm2.5,ozone,greenness,mental_class))

set.seed(1993)
n_test <- round(nrow(dat_class)/6)
test_indices <- sample(1:nrow(dat_class), n_test, replace=FALSE)
test_dat <- dat_class[test_indices,]
train_dat <- dat_class[-test_indices,]
```

```{r,,eval=FALSE}
write_csv(train_dat,"train.csv")
write_csv(test_dat,"test.csv")
```

try random forest for new generated test and train dataset:
```{r}
test_dat = read_csv("test.csv")
train_dat = read_csv("train.csv")
```

viewing the class in 2D via our 2 exposures of interest (ozone and greenness):
```{r,warning=FALSE}
p <- train_dat %>% 
  filter(!is.na(mental_class)) %>%
  ggplot(aes(ozone, greenness)) +
  geom_point(aes(color = factor(mental_class))) +
  ggtitle("Viewing the mental_class in 2D plot with ozone and greenness")
colors <- c("lightcoral", "cornflowerblue")
p + scale_color_manual(values=colors)
```

try CART:
```{r}
library(stringr)
library(lubridate)
library(tidyr)
library(XML)
library(tree)
fit1 <- train_dat %>%
  filter(!is.na(mental_class)) %>%
  mutate(mental_class = factor(mental_class)) %>%
  tree(mental_class ~ ., data = .)

plot(fit1)
text(fit1)

```

try Random Forest
```{r}
library(randomForest)
train_dat_com = train_dat[complete.cases(train_dat),]
train_dat_com$mental_class = as.factor(train_dat_com$mental_class)


fit2 <- randomForest(mental_class ~., data = train_dat_com, ntree=500, nodesize = 15, mtry=1)
```

```{r}
test_dat_com = test_dat[complete.cases(test_dat),]
test_dat_com$mental_class = as.factor(test_dat_com$mental_class)
pred <- predict(fit2, newdata = test_dat_com, type = "class")


tab <- table(pred, test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

```{r,,eval=FALSE}
fit3 <- randomForest(mental_class ~., data = train_dat_com, ntree=700, nodesize = 10, mtry=1)

pred <- predict(fit3, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

try to choose best node size:
```{r,eval=FALSE}
node_size = seq(1,20,1)
accuracy = rep(0,20)
for(i in 1:20) {
  fit = randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = node_size[i], mtry=1)
  pred <- predict(fit, newdata = test_dat_com, type = "class")
  tab <- table(pred = pred, true = test_dat_com$mental_class)
  accuracy[i] = confusionMatrix(tab)$overall["Accuracy"]
}
node_size[which.max(accuracy)]
```

```{r,eval=FALSE}
set.seed(1)
fit1 <- randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = 13, mtry=3)

pred <- predict(fit1, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
```

try to choose best mtry:

```{r,eval=FALSE}
set.seed(1)
node_size = seq(1,20,1)
mtry_n = seq(1,15,1)
accuracy = c()
for(i in 1:20) {
  for (j in 1:15) {
    fit = randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = node_size[i], mtry=mtry_n[j])
  pred <- predict(fit, newdata = test_dat_com, type = "class")
  tab <- table(pred = pred, true = test_dat_com$mental_class)
  accuracy = c(confusionMatrix(tab)$overall["Accuracy"],accuracy)
  }
}
```


```{r}
set.seed(1)
fit2 <- randomForest(mental_class ~., data = train_dat_com, ntree=1000, nodesize = 15, mtry=5)

pred <- predict(fit2, newdata = test_dat_com, type = "class")


tab <- table(pred = pred, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]

pred <- predict(fit2, newdata = test_dat_com,type = "prob")


tree.prob = pred[,2]
#accuracy 0.756371 
```

In order to find the important variables in the random forest:

```{r,eval=FALSE}
library(knitr)
variable_importance <- importance(fit1) 
tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
  arrange(desc(Gini))
```

```{r,eval=FALSE}
ds_theme_set()
tmp %>%
  ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
  geom_bar(stat='identity',fill = "darkseagreen2") +
  coord_flip() + xlab("Feature") +
  theme(axis.text=element_text(size=8)) + 
  ggtitle("mean feature importances for random forest classifer")
```



try to use lda and qda for classification:
```{r}
library(MASS)
lda.fit=lda(mental_class~.,data=train_dat_com)
lda.fit
lda.pred=predict(lda.fit, test_dat_com)
lda.class=lda.pred$class 
tab <- table(pred = lda.class, true = test_dat_com$mental_class)
tab
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]
#0.756371 

lda.prob = lda.pred$posterior[,1]
```

```{r}

qda.fit=qda(mental_class~.,data=train_dat_com)
qda.fit

qda.pred=predict(qda.fit, test_dat_com)

qda.class=qda.pred$class 
tab = table(qda.class,test_dat_com$mental_class)
confusionMatrix(tab)$tab
confusionMatrix(tab)$overall["Accuracy"]

qda.prob = qda.pred$posterior[,1]

#0.7319062 
```

## Logistic Regression (2 exposure)
```{r}
glm_fit <- train_dat_com %>% 
  mutate(y = as.numeric(mental_class == 1)) %>%
  glm(y ~ income7 + employed + out_of_work + homemaker + student + hlthplan + exercise + smoke + drink + genhlth_fa + genhlth_po + qlactlm + pm2.5 + ozone + greenness, data=., family = "binomial")
```

obtain prediction using the predict function:

```{r}
p_hat_logit <- predict(glm_fit, newdata = test_dat_com, type="response")
```

obtain predictions

```{r}
y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0)
confusionMatrix(data = y_hat_logit, reference = test_dat_com$mental_class)
#0.7528 
```

## KNN

```{r,eval=FALSE}
accuracy <- c()
for(x in 1:100){
knn_fit <- knn3(mental_class~. ,data = na.omit(train_set), k=x)
f_hat <- predict(knn_fit, newdata = na.omit(test_set))[,2]
tab <- table(pred=round(f_hat), truth=na.omit(test_set)$mental_class)
accuracy[x] <- confusionMatrix(tab)$overall["Accuracy"]
}
x <- 1:100
data.frame(x, accuracy) %>% ggplot(aes(x, accuracy))+
  geom_point(aes())+
  geom_line()+
  geom_vline(xintercept=x[which.max(accuracy)], col = "red")
```

so, k=35

```{r}
knn_fit <- knn3(mental_class~. ,data = na.omit(train_dat_com), k=35)
f_hat <- predict(knn_fit, newdata = na.omit(test_dat_com))[,2]
tab <- table(pred=round(f_hat), truth=na.omit(test_dat_com)$mental_class)
confusionMatrix(tab)$overall["Accuracy"]

#0.7415902 
```


draw the ROC curve:

```{r}


roc_ran <- roc(test_dat_com$mental_class,tree.prob)
auc(roc_ran)
#0.8106
roc_qda <- roc(test_dat_com$mental_class, qda.prob)
auc(roc_qda)
#0.7757
roc_lda = roc(test_dat_com$mental_class, lda.prob)
auc(roc_lda)
#0.79
roc_lg = roc(test_dat_com$mental_class, p_hat_logit)
auc(roc_lg)
#0.7919

roc_knn = roc(test_dat_com$mental_class, f_hat)
auc(roc_knn)
#0.7852

plot(roc_ran,col = "brown1")
lines(roc_qda, col = "blue1")
lines(roc_lda,col = "darkgoldenrod")
lines(roc_lg , col = "darkorchid")
lines(roc_knn, col = "chartreuse1")
legend(0.4,0.5,c("Random Forest","QDA","LDA","LG","KNN"),col=c("brown1","blue1","darkgoldenrod","darkorchid","chartreuse1"),lty=1)
```

try to use regression tree for regression:

```{r}
library(tree)
data_all = read_csv("data_3_expo.csv")
data_tree = data_all %>%
  dplyr::select(income7,employed,out_of_work,homemaker,student,hlthplan,exercise,smoke,drink, genhlth_fa,genhlth_po,qlactlm,pm2.5,ozone,greenness,menthlth)
fit_tree <- tree(menthlth ~ ., data = data_tree)
plot(fit_tree)
text(fit_tree, cex = 0.5)
```

